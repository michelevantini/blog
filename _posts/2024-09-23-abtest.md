---
title: "Experimentation and A/B testing"
permalink: /abtest
date: 2024-09-23
---

When it comes to experimentation and A/B testing, it seems that a lot of people focus on methods for calculating  statistical significance (are you using p-values or a Bayesian approach, etc.). However, you will quickly realize that it is probably a fairly small part of your entire experimentation platform and culture.

A book that has helped me tremendously in my efforts towards improving experimentation practices is [Trustworthy Online Controlled Experiments](https://www.cambridge.org/core/books/trustworthy-online-controlled-experiments/D97B26382EB0EB2DC2019A7A7B518F59#fndtn-metrics)


## 1. Approaches
The two most common ways of testing for statistical significance are essentially two:
 - [Calculating p-values](https://medium.com/analytics-vidhya/a-b-testing-simple-explanation-of-maths-behind-a3b2f059e619)
 - [Applying Bayesian statistics](https://vwo.com/downloads/VWO_SmartStats_technical_whitepaper.pdf#page=21.15)

But why would you use one or the other, or maybe both? The most common arguement that you will find against p-values is that it's harder to communicate to non-technical people. Essentially because people try to explain as "Assuming that the null hypothesis is true, what are the chances of obtaining data that are at least this extreme". In practice, when you set that the p-value has to be less than or equal to 0.05, you are essentially accepting a 5% false positive rate. Meaning that making decisions based on this value, will result on average on 95% of correct decisions and 5% of wrong decisions. I think this latter explanation is far more reasonable to communicate and to be understood.

Nontheless, when we switch to Bayesian statistics, we can answer more direct questions, such as:
 - what is the probability that this variant is better than control?
 - If I choose this variant, when it is actually worse than control, how much will I loose?

These of course are more straightforward to interpret and can save us (to some extent) from discussions about statistical power and sample size. This mostly because in Bayesian statistics, everything comes as a probability or interval "automatically".

### 1.1 p-values
### 1.2 Bayesian statisitcs
For an in-dept introduction to Bayesian modeling, I recommend the Bayesian Data Analysis course from Aalto University (you can access the video lectures [here](https://aalto.cloud.panopto.eu/Panopto/Pages/Sessions/List.aspx#folderID=%224a7f385e-fdb1-4382-bfd0-af0700b7fc46%22)).

Let's now cover the two most common models we need in A/B testing: a model for binary data (such as conversion rates) and a model for continuous data (such as watchtime).

The most simple model for binary data is the Beta-Binomial model, which uses conjugate priors. The conversion rate has a Beta prior, which can become a Uniform distribution when $\alpha=1$ and $\beta=1$. Given the conversion rate, the count of converted user has a binomial distribution. So we can directly derive the posterior distribution of the conversion rate given the data:
$$ P_A \sim Beta(\alpha, \beta) $$
$$ X_A \vert P_A \sim Bin(n_A, p_A) $$
$$ P_A \vert X_A \sim Beta(\alpha + x_A, \beta + n_A - x_A) $$

where $n_A$ is the total number of users and $x_A$ is the actual number of converted users.

For continuous data, an option is the Normal-Normal model.


Other resources:
 - [Beyond Simple A/B Testing: Advanced Experimentation Tactics](https://www.youtube.com/watch?v=LOhvpOFAlf4&t=1s&ab_channel=DataCouncil)
 - [Under the Hood of Uberâ€™s Experimentation Platform](https://www.uber.com/en-FI/blog/xp/)