---
title: "Experimentation and A/B testing"
permalink: /abtest
date: 2024-09-23
---

When it comes to experimentation and A/B testing, it seems that a lot of people focus on methods for calculating statistical significance (are you using p-values or a Bayesian approach, etc.). However, you will quickly realize that it is probably a fairly small part of your entire experimentation platform and culture.

A book that has helped me tremendously in my efforts towards improving experimentation practices is [Trustworthy Online Controlled Experiments](https://www.cambridge.org/core/books/trustworthy-online-controlled-experiments/D97B26382EB0EB2DC2019A7A7B518F59#fndtn-metrics)

We will first introduce methods to calculate statistical significance. This is useful for introducing specific terminology and concepts. The rest of the article will cover practical considerations to running A/B tests.

## 1. Statistical significance methods
The two most common ways of testing for statistical significance are essentially two:
 - [Calculating p-values](https://medium.com/analytics-vidhya/a-b-testing-simple-explanation-of-maths-behind-a3b2f059e619)
 - [Applying Bayesian statistics](https://vwo.com/downloads/VWO_SmartStats_technical_whitepaper.pdf#page=21.15)

But why would you use one or the other, or maybe both? The most common arguement that you will find against p-values is that it's harder to communicate to non-technical people. Essentially because people try to explain as "Assuming that the null hypothesis is true, what are the chances of obtaining data that are at least this extreme". In practice, when you set that the p-value has to be less than or equal to 0.05, you are essentially accepting a 5% false positive rate. Meaning that making decisions based on this value, will result on average on 95% of correct decisions and 5% of wrong decisions. I think this latter explanation is far more reasonable to communicate and to be understood.

Nontheless, when we switch to Bayesian statistics, we can answer more direct questions, such as:
 - what is the probability that this variant is better than control?
 - If I choose this variant, when it is actually worse than control, how much will I loose?

These of course are more straightforward to interpret and can save us (to some extent) from discussions about statistical power and sample size. This mostly because in Bayesian statistics, everything comes as a probability or interval "automatically".

### 1.1 p-values
In [this article](https://medium.com/analytics-vidhya/a-b-testing-simple-explanation-of-maths-behind-a3b2f059e619), you can find a rather clean and simple explanation of how to calculate p-values. 

We usually set a threshold for p-values to claim statistical significance. A good standard id p-value < 0.05, which means that we are willing to accept a False Positive Rate < 0.05 (also known as Type I errors). So that chance of us claiming statistical significance, when in fact that is not the case, is <5%.

You will alwasy have a trade off between False Positive Rate(FPR or Type I error) and False Negative Rates (FNR or Type II error). Meaning that if you for example decides to lower the bar to p-values < 0.1 then you are accepting higher FPR, but you are lowering FNR, so the probability of detecting a statistically significant difference when there really is one is higher.

The concept of FPR also allow us to define statistical power as $Power = 1 - FPR$, which we can further parametrize in terms of the minimum delta of interest $\delta$ as 

$$ Power_\delta = P(\vert T \vert \geq 1.96 \vert \Delta = \delta) $$

Where T is the t-statistic and $\Delta$ is the measured difference. Also we are assuming here that we are interested in p-values < 0.05 (hence the 1.96).

This ultimately allows us to define a sample size that allow us to achieve at least a certain power for our test (the industry standard is 80%). Assuming control and treatment have a 50-50 split, then to achieve 80% power we need a number of samples $ n \approx 16\sigma^2/\delta^2 $. Where $\delta^2$ is the sample variance.


### 1.2 Bayesian statisitcs
For an in-dept introduction to Bayesian modeling, I recommend the Bayesian Data Analysis course from Aalto University (you can access the video lectures [here](https://aalto.cloud.panopto.eu/Panopto/Pages/Sessions/List.aspx#folderID=%224a7f385e-fdb1-4382-bfd0-af0700b7fc46%22)).

Let's now cover the two most common models we need in A/B testing: a model for binary data (such as conversion rates) and a model for continuous data (such as watchtime).

The most simple model for binary data is the Beta-Binomial model, which uses conjugate priors. The conversion rate has a Beta prior, which can become a Uniform distribution when $\alpha=1$ and $\beta=1$. Given the conversion rate, the count of converted user has a binomial distribution. So we can directly derive the posterior distribution of the conversion rate given the data:

$$ P_A \sim Beta(\alpha, \beta) $$

$$ X_A \vert P_A \sim Bin(n_A, p_A) $$

$$ P_A \vert X_A \sim Beta(\alpha + x_A, \beta + n_A - x_A) $$

where $n_A$ is the total number of users and $x_A$ is the actual number of converted users. Setting a $Beta(\alpha, \beta)$ prior distribution is equivalent to $\alpha + \beta - 2$ prior observations (indeed when we have a $Beta(1, 1)$, meaning a Uniform distribution, we add no information). That should give you an idea of how different parameters for the Beta prior affect the posterior. As an alternative, you can find [here](https://vwo.com/downloads/VWO_SmartStats_technical_whitepaper.pdf#page=21.15) the derivations of how different values of $\alpha$ and $\beta$ relate to the mean and variance of the distribution.

 

For continuous data, an option is the Gamma-Exponentional model.

[AB Testing With Continuous Outcomes (And Horribly Misspecified Priors)](https://making.lyst.com/2014/09/04/testing-continuous-variables/)


## 2. Practical consideration to run an A/B test
Things that anyone should remember when doing statistics or looking at any kind of data:
 - Compare apples to apples
 - Twyman's law: "Any figure that looks interesting or different is usually wrong"

### 2.1 Validity
Common threats to your test validity:
 - **SUTVA** (Stable Unit Treatment Value Assumption) experiment units should not interfere with each other
 - **Survivorship bias**: analyzing users who survived might not exactly tell you why the other ones left
 - **Intention to treat**: filtering out users who do not make use of a new feature/change introduces selection bias
 - **SRM** (Sample Ratio Mismatch): A small imbalance can invalidate the test. For example, if one variant is getting less users than the others you might also get less extreme/heavy users, which will result in a misinterpretation of the results.
 - **Residual/carryover effects**: new experiments means new code, which tends to introduce new bugs that can affect even later tests
 - **Populations**: it's better to not generalize across segments (for example different countries). If you first deploy your test in one country, re-validate your results for other countries
 - **Time**: generalizing along the time axis is hard. If you need to test long-term effects, consider using holdout experiments
 - Other time-related threats to validity:
    - **Primacy effects**: new features take time to be adopted by users. This can delay their effect on key metrics. A specific case of this involves preiodically re-training a Machine Learning model, which might get better over time
    - **Novelty effects**: new features might become the new shiny object for a short period of time, but their usage might decline rapidly over time
    - These can be detected by plotting the treatment effect over time to check for anomalies. If you fear primacy or novelty effects to be present, you might want to run the test for longer to check if the treatment effect stabilizes

### 2.2 Segments
What constitutes a good segment?
 - **Country or market**: different features might be more effective in different markets/countries. Beware of potential effects derived from poor localization (such as, bad translations)
 - **Platform**: browser/desktop/mobile, iOS/Android; sometime some of these segments "naturally" behave better. For example, iPhones are typically high-end devices, so the experience on them will be likely better and they are also a reflection of the users who buy them.
 - **Time of day/day of week**: beware of the patterns of different metrics over time
 - **User type**: new or existing
 - **Any app-specific user account characteristics**: for example, is this a family account on Spotify
 
 


Other resources:
 - [Beyond Simple A/B Testing: Advanced Experimentation Tactics](https://www.youtube.com/watch?v=LOhvpOFAlf4&t=1s&ab_channel=DataCouncil)
 - [Under the Hood of Uberâ€™s Experimentation Platform](https://www.uber.com/en-FI/blog/xp/)