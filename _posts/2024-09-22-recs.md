---
title: "Recommenders and personalization"
permalink: /recs
date: 2024-09-22
---

It appears to me that the literature about recommenders and personalization is essentially split into these categories:
 1. **More theoretical results**: typically proofs of convergence of some method and then tested on some benchmark dataset (for example, [this](https://www.uber.com/en-FI/blog/engineering/data/?uclick_id=268bf0e4-47d9-4154-b2a6-dc5f2e736e82))
 2. **Production experiments with existing or new methods**: company X implements method Y and shows how much that improves target metrics compared to some basiline. Occasionally, the company will also show some new metrics or proxy metrics (for example, the corpus-related metrics presented by DeepMind and experimented on YouTube Shorts [here](https://arxiv.org/html/2305.07764v2))
 3. **Engineering-related aspects of recommenders**: data engineering and practical machine learning engineering consideration to run recommenders more in real-time at scale (a good example is the [TikTok recommender paper](https://arxiv.org/pdf/2209.07663))
 4. A mix of the above

Assuming you are a data scientist or ML engineer willing to master recommender systems, I believe you should be interested in all of them, with probably a focus on 2,3 and 4. In recommenders, theoretical results are important, but the actual large-scale production tests run by for example FAANG companies can be sometime more relevant. You can use this kind of literature to get new ideas for an A/B test, mix and match different kind of approaches and to have a general idea of where the industry is going. Results will most likely not directly translate to you problem, no matter how similar they are, but they can be great source on inspiration and knowledge. With this in mind, through this article we will focus mostly on practical aspects of recommender systems and methods.

A quite exhaustive overview of important topics in Recommender systems are shown in [here](https://user-images.githubusercontent.com/50820635/85274861-7e0e3b00-b4ba-11ea-8cd3-2690ec55a67a.jpg) - [Credits](https://github.com/jihoo-kim/awesome-RecSys?tab=readme-ov-file) - We will roughly follow the structure reported there, with some adjustments.

## 1. Design
In practice, a recommender system is rarely composed of a single component that handles everything. The general approach to recommendations is to split the process in stages, with different components doing more specialized jobs. The general architecture of a morern recommender system is:
1. **Candidate generation** or **Retrieval stage**: different methods can be used to generate a fairly big pool of possible recommendations for a user. This can include more algorithmic approaches, such as nearest neighbours methods applied in an embedding space generated by a Collaborative filtering method, or more rule-based sets, such as content from the people you follow.
2. **One or more stages of Re-ranking**: the re-ranking step typically involves using user and item features to order the candidate items (generated in the first stem) to provide a more personalized experience to the user. Let's assume that the Candidate generation step generated 1000 items, the subsequent re-ranking step can first order all the 1000 items, and then additional re-ranking can be applied to the top 100 or top 10. Re-ranking adds personalization but is more expensive in terms of latency. Hence, one can apply a more lightweight model for all the 1000 items and a more complex model to rank only the top 10 items. 
3. **Additional business rules**: for example removing sensitive content or duplicate content

For example, here is an explanation of [Instagram retrieval and re-ranking steps](https://engineering.fb.com/2023/08/09/ml-applications/scaling-instagram-explore-recommendations-system/)

## 2. Methods
In this section we will look at first at more traditional methods, such as Collaborative filtering, and then dive into more recent Reinforcement Learning based approaches.

Recommender systems are typically categorised according to the following taxonomy:
 - **Content-based filtering methods**: given some features for users and items, define a model on user-item interactions
 - **Collaborative filtering methods**: just define a model on user-item interactions and user/item representations have to be learned
 - Hybrid methods

### 2.1 Collaborative filtering
A fearly simple to implement yet effective baseline is the Collaborative filtering approach, where we learn user/item representations from an interaction matrix. The learned representations can then be used to make predictions. Most commonly used Collaborative filtering methods include:
 - **Matrix factorization**: [a simple introduction](https://developers.google.com/machine-learning/recommendation/overview/candidate-generation)
 - **Neural Collaborative Filtering**
 - **Factorization Machines**: which are mostly considered Collaborative filtering methods since they model a user-item interaction matrix. But they can handle user and item features, so they are also a Content based method 

So now that we have two embedding spaces, one for the users and one for the items. We can perform recommendations mostly of two kinds:
 - Similar items to what the user interacted with in the past
 - Items that similar users interacted with
This last point in particular, can greatly help with the the discovery of new user interests. The system might recommend something new because a similar user interacted with it.

#### 2.1.1 Matrix Factorization (MF)
The idea is to decompose a user-items interaction matrix A (n x m) into two embedding matrices U (n x d) and V (m x d), such that the dot-product of $ u_i$  and $ v_j $  is a good approximation of $ a_{ij} $ . With millions of items and users, there is a significant benefit in learning and using the matrix factorization compared to the full matrix.

**Objective Function** 

Weighted Matrix Factorization: 

$$ \min_{U,V} \sum_{(i,j)\in obs} w_{ij}(A_{ij} - \left< U_i, V_j \right>)^2 + w_0 \sum_{(i,j)\notin obs} \left< U_i, V_j \right>^2 $$ 

where $ w_0 $  is a hyperparameter and $ w_{ij} $  allows us to weight training example differently. With $ w_{ij} $  we can account for very popular items or users and andjut their weight by their frequency such that their don't overpower the objective function.

This can be optimized either with Stochastic Gradient Descent (SDG) or WALS (Weighted Alternating Least Square). SDG has the main advantage of giving the option to choose different loss functions, while WALS typically converges faster.

**Cold-start problem**
For new users or items we cannot have immediately their embeddings. We can solve that with the following techniques:
 - Given some interactions for a new item, we can compute an embedding $ v_{i_0} $  for it as $ \min_{v_{i_0}} \Vert A_{i_{0}} - U v_{i_0} \Vert $ , by fixing U and solving for $ v_{i_0} $ . Similarly, this also works for user embeddings
 - We can try to approximate an item embedding by the average embedding of items in the same category/creator/etc.

**Libraries and tools**
A very easy to use implementation of matrix factorization solved through the WALS algorithm, is implemented in the Python package [implicit](https://benfred.github.io/implicit/). The implementation in the implicit library follows the implementation described [here](http://yifanhu.net/PUB/cf.pdf), which is slightly different from the one showed above: 

$$ \min_{U,V} \sum_{i,j} c_{ij}(p_{ij} - \left< U_i, V_j \right>)^2 + \lambda (\sum_{i} \Vert U_i \Vert^2 + \sum_{j} \Vert V_i \Vert^2) $$

#### 2.1.2 Neural Collaborative filtering
The main downside of using Matrix factorization is the difficulty of using a set of hand-engineered features for users or items. Also, with Matrix factorization, recommendations tend to be dominated by very popular items. With Deep Neural Network (DNN) models, we can address these problems by adding specific set of features, hence improving relevance of recommendations.

The flexibility of DNNs allow us to include in the input:
 - Dense features (such as watch time, time since last watch, etc.)
 - Sparse features (such as watch history and country)

The output is typically some sort of softmax layer of the same length of the corpus of items (or a subset of relevant items). So given the softmax function $ h$  output of the last layer $ \phi{x} $ , we get a probability distribution $ \widehat{p} = h(\phi{x}V^T) $ , where $ V $  is the weight matrix of the softmax layer. In these settings, since $ \log{\widehat{p}} =  \left< \phi{x}, V_j \right>) + \log{Z} $  (where $ Z $  is a normalization constant), we can interpret $ \phi{x} $  and $ V_j $  to be user and item embeddings. The higher the dot product $ \left< \phi{x}, V_j \right> $ , the higher the probability $ \widehat{p_j} $ , that's our similarity measure in the embedding space.

##### 2.1.2.1 Neural Matrix Factorization (NMF)
A first model that belongs to this category is NeuMF, a Neural Collaborative Filtering model introduced in 2017 ([He et al., 2017](https://arxiv.org/pdf/1708.05031)). At first they introduce the framework for Neural Collaborative Filtering. This essentially means that you can use embeddings layers for users and items in a Neural Network (NN) with Mean Squared Error (MSE) loss to reproduce a traditional MF model. They then extend this approach introducing the NeuMF architecture by adding separate users and items embeddings layers which are then concatenated and used as input of a sequence of linear layers that then merges with the MF branch of the network. This other branch is an attempt to capture possibly different interaction effects between users and items that are not learned in the MF branch.  

##### 2.1.2.1 Deep Factorization Machines (DeepFM)

This idea further extends to the so-called [Two-Tower model](https://research.google/pubs/sampling-bias-corrected-neural-modeling-for-large-corpus-item-recommendations/) when we want to also include item features. Instead of using a softmax layer, where we learn a probability for each item in the corpus, we learn two separate mapping $ \phi{x_{user}} $   and $ \psi{x_{item}} $  and the output of the model is simply their dot product $ \left< \phi{x_{user}}, \psi{x_{item}} \right> $  or any other similarity measure (for example, cosine similairty). 


### 2.2 Content-based filtering

#### 2.2.1 Bandits algorithms
The idea Multi-Armed Bandits is inspired from the idea of gambling in a casino. A gambler faces several slot machines, or “one-armed bandits”, each with a unknown probability of getting a reward. The goal is to play and maximize the total reward.

Bandits algorithms are a very effective yet simple way of balancing exploration (explore new items) and exploitation (exploit previous interactions to recommend items with a high level of confidence). Typically, this models are fairly lightweight, such as a Contextual bandit algorithm based on linear regression and $\epsilon$-greedy exploration, so they allow for near real-time update of preferences. This means that they are able to quickly adapt to new information and react fast.

For these reasons, Multi-Armed Bandits can also be used in the context of A/B testing where each arm is a variant in the experiment. We will discuss this application in a separate post.

In particular, Contextual Bandits have become popular over the year. [Contextual Bandits](https://arxiv.org/pdf/1003.0146) are Content-based filtering methods that take in either a user vector or a user and an item vector and generate recommendations. The so-called context in this scenario is the user features vector. Different Bandits methods differ in the way they implement the exploration vs. exploitation mechanism, the most popular being $\epsilon$-greedy, UCB and Thompson Sampling.

Extensive discussion of these methods can be found [here](https://eugeneyan.com/writing/bandits/) and [here](https://vinija.ai/recsys/multi-armed-bandit/#contextual-bandit-algorithms).

Many professionals from high-profile companies are very active in this ares and occasionally publish very high quality reviews with many practial tips, such as this talk [An industry perspective on Bandit Feedback](https://sites.google.com/view/practical-bandits-tutorial).

### 2.3 Reinforcement-learning approaches
The basic idea of using Reinforcement-Learning (RL) in recommenders is to have an agent (our recommender) that tries to take actions (make recommendations) in an environment (our platform/website) where our users exist, with the goal of optimizing a specific metric. One benefit of this approach, is that compared to a Collaborative filtering method where we probably need some differentiable target to optimize, in RL-based methods we can sometimes directly optimize business metrics or proxy metrics.

We can further split these approaches into sub-categories:
 - **Bandits approaches**
 - **Deep RL methods**
 - **Hybrid methods** 

While Deep RL methods are an [active field or research](https://scitator.medium.com/rl-in-recsys-an-overview-e02815019a8f) and they did show positive results on real-world large scale personalization scenarios, it seems that the most commonly used methods are Collaborative filtering, Factorization machines and Bandits algorithms. We will not dive here into Deep RL methods, we will instead explore Bandits algorithms and their applications.

## 3. Use-cases
Different use-cases have specific requirements:
 - News recommendations: news is information that is consumed quickly and that also becomes irrelevant fast. You might want to implement something lightweight and that can react fast, such as a Contextual Bandits
 - Social media content: in apps like Instagram or TikTok, you need to take into account several factors: fresh content is probably preferred but old content can still be valid, users are on a social graph (so you need to additionally take into account the activity of a user's circle), etc. Again, I like to refer to [Instagram implementation details](https://sites.google.com/view/practical-bandits-tutorial), where they approach this by gathering content from multiple sources and then apply multiple stages or re-ranking
 - Content recommendations of the kind of Netflix and Spotify can probably get away with some kind of hybrid of collaborative filtering and content-based filtering. For example you could generate candidates with a collaborative filtering method, then the order of the horizontal bars in Netflix could be ordered by a Contextual bandit and finally you can apply re-ranking to re-order the content of each bar to personalize the experience of the specific user. In comparison, Netflix and Spotify are "slower" in terms of content creation and consumption than TikTok and Instagram
 - Video recommendations, such as YouTube, have some of the properties of all the platforms above. Fresh content is important, but older content is also very relevant, and there is a fairly strong network component
 - Videogame recommendations, such as HypeHype, also tend to be more like YouTube. You can observe trends that come and go, but a videogame is "re-usable". So in that sense is more like a Netflix series, you will go back to it for a while and move on when it's "completed", or in the case of the videogame, you probably feel like you played enough
 - Uber and food delivery systems are quite different in nature. A lot of the recommendations and business rules are more oriented into creating an "economy", and you typically have hard constraints, such as geographical location, price, etc.




List of useful resources:
 - [Netflix blog](https://netflixtechblog.com/)
 - [Spotify blog](https://engineering.atspotify.com/)
 - [Uber blog](https://www.uber.com/en-FI/blog/engineering/data/?uclick_id=268bf0e4-47d9-4154-b2a6-dc5f2e736e82)
 - [Linkedin Recommenders blog](https://www.linkedin.com/blog/engineering/recommendations)
