---
title: "Recommenders and personalization"
permalink: /recs
date: 2024-09-22
---

It appears to me that the literature about recommenders and personalization is essentially split into these categories:
 1. More theoretical results: typically proofs of convergence of some method and then tested on some benchmark dataset (for example, [this](https://www.uber.com/en-FI/blog/engineering/data/?uclick_id=268bf0e4-47d9-4154-b2a6-dc5f2e736e82))
 2. Real-world tests with existing or new methods: company X implements method Y and shows how much that improves target metrics compared to some basiline. Occasionally, the company will also show some new metrics or proxy metrics (see for example are the corpus-related metrics presented by DeepMind and experimented on YouTube Shorts [here](https://arxiv.org/html/2305.07764v2))
 3. Engineering-related aspects of recommenders: data engineering and practical machine learning engineering consideration to run recommenders more in real-time at scale (a good example is the [TikTok recommender paper](https://arxiv.org/pdf/2209.07663))
 4. A mix of the above

Assuming you are a data scientist or ML engineer, I believe one should be interested in all of them, with probably a focus on 2,3 and 4. 
Through this article we will focus mostly on practical aspects of recommender systems and methods.

A quite exhaustive overview of important topics in Recommender systems are shown in [here](https://user-images.githubusercontent.com/50820635/85274861-7e0e3b00-b4ba-11ea-8cd3-2690ec55a67a.jpg) - [Credits](https://github.com/jihoo-kim/awesome-RecSys?tab=readme-ov-file) - We will roughly follow the structure reported there, with some adjustments.

## 1. Design
In practice, a recommender system is rarely composed of a single component that handles everything. The general approach to recommendations is to split the process in stages, with different components doing more specialized jobs. The general architecture of a morern recommender system is:
1. Candidate generation or Retrieval stage: different methods can be used to generate a fairly big pool of possible recommendations for a user. This can include more algorithmic approaches, such as nearest neighbours methods applied in an embedding space generated by a Collaborative filtering method, or more rule-based sets, such as content from the people you follow.
2. One or more stages of Re-ranking: the re-ranking step typically involves using user and item features to order the candidate items (generated in the first stem) to provide a more personalized experience to the user. Let's assume that the Candidate generation step generated 1000 items, the subsequent re-ranking step can first order all the 1000 items, and then additional re-ranking can be applied to the top 100 or top 10. Re-ranking adds personalization but is more expensive in terms of latency. Hence, one can apply a more lightweight model for all the 1000 items and a more complex model to rank only the top 10 items. 
3. Additional business rules: for example removing sensitive content or duplicate content

For example, here is an explanation of [Instagram retrieval and re-ranking steps](https://engineering.fb.com/2023/08/09/ml-applications/scaling-instagram-explore-recommendations-system/)

## 2. Methods
In this section we will look at first at more traditional methods, such as Collaborative filtering, and then dive into more recent Reinforcement Learning based approaches.

Recommender systems are typically categorised according to the following taxonomy:
 - Content based methods: given some features for users and items, define a model on user-item interactions
 - Collaborative filtering methods: just define a model on user-item interactions and user/item representations have to be learned
 - Hybrid methods

### 2.1 Collaborative filtering
A fearly simple to implement yet effective baseline is the Collaborative filtering approach, where we learn user/item representations from an interaction matrix. The learned representations can then be used to make predictions and they follow the basic idea "users that liked this also liked this". Most commonly used Collaborative filtering methods include:
 - Matrix factorization: [a simple introduction](https://developers.google.com/machine-learning/recommendation/overview/candidate-generation)
 - Neural Collaborative Filtering
 - Factorization Machines: which are mostly considered Collaborative filtering methods since they model a user-item interaction matrix. But they can handle user and item features, so they are also a Content based method 

### 2.2 Reinforcement-learning approaches
The basic idea of using Reinforcement-Learning (RL) in recommenders is to have an agent (our recommender) that tries to take actions (make recommendations) in an environment (our platform/website) where our users exist, with the goal of optimizing a specific metric. One benefit of this approach, is that compared to a Collaborative filtering method where we probably need some differentiable target to optimize, in RL-based methods we can sometimes directly optimize business metrics or proxy metrics.

We can further split these approaches into sub-categories:
 - Bandits approaches
 - Deep RL methods
 - Hybrid methods 

While Deep RL methods are an [active field or research](https://scitator.medium.com/rl-in-recsys-an-overview-e02815019a8f) and they did show positive results on real-world large scale personalization scenarios, it seems that the most commonly used methods are Collaborative filtering, Factorization machines and Bandits algorithms. We will not dive here into Deep RL methods, we will instead explore Bandits algorithms and their applications.

#### 2.2.1 Bandits algorithms
The idea Multi-Armed Bandits is inspired from the idea of gambling in a casino. A gambler faces several slot machines, or “one-armed bandits”, each with a unknown probability of getting a reward. The goal is to play and maximize the total reward.

Bandits algorithms are a very effective yet simple way of balancing exploration (explore new items) and exploitation (exploit previous interactions to recommend items with a high level of confidence). Typically, this models are fairly lightweight, such as a Contextual bandit algorithm based on linear regression and \epsilon-greedy exploration, so they allow for near real-time update of preferences. This means that they are able to quickly adapt to new information and react fast.

For these reasons, Multi-Armed Bandits can also be used in the context of A/B testing where each arm is a variant in the experiment. We will discuss this application in a separate post.

Many professionals from high-profile companies are very active in this ares and occasionally publish very high quality reviews with many practial tips, such as this talk [An industry perspective on Bandit Feedback](https://sites.google.com/view/practical-bandits-tutorial).

List of useful resources:
 - [Netflix blog](https://netflixtechblog.com/)
 - [Spotify blog](https://engineering.atspotify.com/)
 - [Uber blog](https://www.uber.com/en-FI/blog/engineering/data/?uclick_id=268bf0e4-47d9-4154-b2a6-dc5f2e736e82)
